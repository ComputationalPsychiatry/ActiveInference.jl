var documenterSearchIndex = {"docs":
[{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"EditURL = \"../julia_files/TMazeSimulationExample.jl\"","category":"page"},{"location":"TMazeSimulationExample/#Simulation-Example-T-Maze","page":"T-Maze Simulation","title":"Simulation Example T-Maze","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"We will start from the importing of the necessary modules.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"using ActiveInference\nusing ActiveInference.Environments","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"We will create a T-Maze environment with a probability of 0.9 for reward in the the reward condition arm. This is a premade environment in the ActiveInference.jl package.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"env = TMazeEnv(0.9)\ninitialize_gp(env)","category":"page"},{"location":"TMazeSimulationExample/#Creating-the-Generative-Model","page":"T-Maze Simulation","title":"Creating the Generative Model","text":"","category":"section"},{"location":"TMazeSimulationExample/#The-Helper-Function","page":"T-Maze Simulation","title":"The Helper Function","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"When creating the generative model we can make use of the helper function, making it convenient to create the correct structure for the generative model parameters.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"To use the helper function we need to know the following:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"Number of states in each factor of the environment\nNumber of observations in each modality\nNumber of controls or actions in each factor\nPolicy length of the agent\nInitial fill for the parameters","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"Let's start with the factors of the environment. Let's take a look at the T-Maze environment:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"(Image: image1)","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"We here have two factors with the following number of states:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":" Location Factor  Reward Condition Factor\n1. Centre 1. Reward Condition Left\n2. Left Arm 2. Reward Condition Right\n3. Right Arm  \n4. Cue  ","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"We will define this as a vector the following way:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"n_states = [4, 2]","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"We will now define the modalities:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":" Location Modality  Reward Modality  Cue Modality\n1. Centre 1. No Reward 1. Cue Left\n2. Left Arm 2. Reward 2. Cue Right\n3. Right Arm 3. Loss  \n4. Cue    ","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"Here we have 3 modalities, with 4, 3, and 2 observations in each. We will define this as a vector the following way:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"n_observations = [4, 3, 2]","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"Now, let's take a look at the actions, or controls:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":" Controls Location Factor  Controls Reward Condition Factor\n1. Go to Centre 1. No Control\n2. Go to Left Arm  \n3. Go to Right Arm  \n4. Go to Cue  ","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"As we see here, the agent cannot control the reward condition factor, and it therefore believes that there is only one way states can transition in this factor, which is independent of the agent's actions. We will define this as a vector the following way:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"n_controls = [4, 1]","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"Now we can define the policy length of the agent. In this case we will just set it to 2, meaning that the agent plans two timesteps ahead in the future. We will just specify this as an integer:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"policy_length = 2","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"The last thing we need to define is the initial fill for the parameters. We will just set this to zeros for now.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"template_type = \"zeros\"","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"Having defined all the arguments that go into the helper function, we can now create the templates for the generative model parameters.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"A, B, C, D, E = create_matrix_templates(n_states, n_observations, n_controls, policy_length, template_type);","category":"page"},{"location":"TMazeSimulationExample/#Populating-the-Generative-Model","page":"T-Maze Simulation","title":"Populating the Generative Model","text":"","category":"section"},{"location":"TMazeSimulationExample/#Populating-**A**","page":"T-Maze Simulation","title":"Populating A","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"Let's take a look at the shape of the first modality in the A parameters:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"A[1]","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"4×4×2 Array{Float64, 3}:\n[:, :, 1] =\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\n[:, :, 2] =\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For this first modality we provide the agent with certain knowledge on how location observations map onto location states. We do this the following way:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# For reward condition right\nA[1][:,:,1] = [ 1.0  0.0  0.0  0.0\n                0.0  1.0  0.0  0.0\n                0.0  0.0  1.0  0.0\n                0.0  0.0  0.0  1.0 ]\n\n# For reward condition left\nA[1][:,:,2] = [ 1.0  0.0  0.0  0.0\n                0.0  1.0  0.0  0.0\n                0.0  0.0  1.0  0.0\n                0.0  0.0  0.0  1.0 ]","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the second modality, the reward modality, we want the agent to be able to infer \"no reward\" with certainty when in the centre and cue locations. In the left and right arm though, the agent should be agnostic as to which arm produces reward and loss. This is the modality that will be learned in this example.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# For reward condition right\nA[2][:,:,1] = [ 1.0  0.0  0.0  1.0\n                0.0  0.5  0.5  0.0\n                0.0  0.5  0.5  0.0 ]\n\n# For reward condition left\nA[2][:,:,2] = [ 1.0  0.0  0.0  1.0\n                0.0  0.5  0.5  0.0\n                0.0  0.5  0.5  0.0 ]","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"In the third modality, we want the agent to infer the reward condition state when in the cue location. To do this, we give it an uniform probability for all locations except the cue location, where it veridically will observe the reward condition state.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# For reward condition right\nA[3][:,:,1] = [ 0.5  0.5  0.5  1.0\n                0.5  0.5  0.5  0.0 ]\n\n# For reward condition left\nA[3][:,:,2] = [ 0.5  0.5  0.5  0.0\n                0.5  0.5  0.5  1.0 ]","category":"page"},{"location":"TMazeSimulationExample/#Populating-**B**","page":"T-Maze Simulation","title":"Populating B","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the first factor we populate the B with determined beliefs about how the location states change depended on its actions. For each action, it determines where to go, without having to go through any of the other location states. We encode this as:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# For action \"Go to Center Location\"\nB[1][:,:,1] = [ 1.0  1.0  1.0  1.0\n                0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0 ]\n\n# For action \"Go to Right Arm\"\nB[1][:,:,2] = [ 0.0  0.0  0.0  0.0\n                1.0  1.0  1.0  1.0\n                0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0 ]\n\n# For action \"Go to Left Arm\"\nB[1][:,:,3] = [ 0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0\n                1.0  1.0  1.0  1.0\n                0.0  0.0  0.0  0.0 ]\n\n# For action \"Go to Cue Location\"\nB[1][:,:,4] = [ 0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0\n                0.0  0.0  0.0  0.0\n                1.0  1.0  1.0  1.0 ]","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the last factor there is no control, so we will just set the B to be the identity matrix.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# For second factor, which is not controlable by the agent\nB[2][:,:,1] = [ 1.0  0.0\n                0.0  1.0 ]","category":"page"},{"location":"TMazeSimulationExample/#Populating-**C**","page":"T-Maze Simulation","title":"Populating C","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the preference parameters C we are not interested in the first and third modality, which we will just set to a vector of zeros for each observation in that modality. However, for the second modality, we want the agent to prefer the \"reward observation\" indexed as 2, and the dislike the \"loss observation\" indexed as 3.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# Preference over locations modality\nC[1] = [0.0, 0.0, 0.0, 0.0]\n\n# Preference over reward modality\nC[2] = [0.0, 3.0, -3.0]\n\n# Preference over cue modality\nC[3] = [0.0, 0.0]","category":"page"},{"location":"TMazeSimulationExample/#Populating-**D**","page":"T-Maze Simulation","title":"Populating D","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the prior over states D we will set the agent's belief to be correct in the location state factor and uniform, or agnostic, in the reward condition factor.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# For the location state factor\nD[1] = [1.0, 0.0, 0.0, 0.0]\n\n# For the reward condition state factor\nD[2] = [0.5, 0.5]","category":"page"},{"location":"TMazeSimulationExample/#Populating-**E**","page":"T-Maze Simulation","title":"Populating E","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the prior over policies E we will set it to be uniform, meaning that the agent has no prior preference for any policy.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# Creating a vector of a uniform distribution over the policies. This means no preferences over policies.\nE .= 1.0/length(E)","category":"page"},{"location":"TMazeSimulationExample/#Creating-the-prior-over-**A**","page":"T-Maze Simulation","title":"Creating the prior over A","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"When creating the prior over A, we use A as a template, by using 'deepcopy()'. Then we multiply this with a scaling parameter, setting the initial concentration parameters for the Dirichlet prior over A, pA.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"pA = deepcopy(A)\nscale_concentration_parameter = 2.0\npA .*= scale_concentration_parameter","category":"page"},{"location":"TMazeSimulationExample/#Creating-Settings-and-Parameters-Dictionary","page":"T-Maze Simulation","title":"Creating Settings and Parameters Dictionary","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the settings we set the 'useparaminfogain' and 'usestatesinfogain' to true, meaning that the agent will take exploration and parameter learning into account when calculating the prior over policies. We set the policy length to 2, and specify modalities to learn, which in our case is the reward modality, indexed as 2.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"settings = Dict(\n    \"use_param_info_gain\" => true,\n    \"use_states_info_gain\" => true,\n    \"policy_len\" => 2,\n    \"modalities_to_learn\" => [2]\n)","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"For the parameters, we just use the default values, but specify the learning rate here, just to point it out.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"parameters = Dict{String, Real}(\n    \"lr_pA\" => 1.0,\n)","category":"page"},{"location":"TMazeSimulationExample/#Initilising-the-Agent","page":"T-Maze Simulation","title":"Initilising the Agent","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"We can now initialise the agent with the parameters and settings we have just specified.","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"aif_agent = init_aif(\n    A, B, C = C, D = D, E = E, pA = pA, settings = settings, parameters = parameters\n);","category":"page"},{"location":"TMazeSimulationExample/#Simulation","page":"T-Maze Simulation","title":"Simulation","text":"","category":"section"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"We are now ready for the perception-action-learning loop:","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"# Settting the number of trials\nT = 100\n\n# Creating an initial observation and resetting environment (reward condition might change)\nobs = reset_TMaze!(Env)\n\n# Creating a for-loop that loops over the perception-action-learning loop T amount of times\nfor t = 1:T\n\n    # Infer states based on the current observation\n    infer_states!(aif_agent, obs)\n\n    # Updates the A parameters\n    update_parameters!(aif_agent)\n\n    # Infer policies and calculate expected free energy\n    infer_policies!(aif_agent)\n\n    # Sample an action based on the inferred policies\n    chosen_action = sample_action!(aif_agent)\n\n    # Feed the action into the environment and get new observation.\n    obs = step_TMaze!(Env, chosen_action)\nend","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"","category":"page"},{"location":"TMazeSimulationExample/","page":"T-Maze Simulation","title":"T-Maze Simulation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"EditURL = \"../julia_files/Simulation.jl\"","category":"page"},{"location":"Simulation/#Simulation-with-ActiveInference.jl","page":"Simulation","title":"Simulation with ActiveInference.jl","text":"","category":"section"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"When simulating with active inference we need a perception-action loop. This loop will perform the following steps:","category":"page"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"The agent will infer the states of the environment based on its generative model and an observation. The inference here is optimized through the minimization of the variational free energy (see Active Inference Theory Perception).\nThe agent will infer the best action based on the minimization of the expected free energy (see Active Inference Theory Action).\nThe agent will perform the action in the environment and receive an observation for use in the next iteration.","category":"page"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"Note: for learning included, look at the section below.","category":"page"},{"location":"Simulation/#The-Perception-Action-loop:","page":"Simulation","title":"The Perception-Action loop:","text":"","category":"section"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"T = n_iterations\n\nfor t = 1:T\n\n    infer_states!(aif_agent, observation)\n\n    infer_policies!(aif_agent)\n\n    chosen_action = sample_action!(aif_agent)\n\n    observation = environment!(env, chosen_action)\n\nend","category":"page"},{"location":"Simulation/#The-Perception-Action-Learning-loop:","page":"Simulation","title":"The Perception-Action-Learning loop:","text":"","category":"section"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"When learning is included, the loop is very similar except for the addition of the update functions, which should be implemented at different points in the loop. Below we will show how to include learning of the parameters. It is important that only the parameters which have been provided to the agent as a prior are being updated.","category":"page"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"T = n_iterations\n\nfor t = 1:T\n\n   infer_states!(aif_agent, observation)\n\n   update_parameters!(aif_agent)\n\n   infer_policies!(aif_agent)\n\n   chosen_action = sample_action!(aif_agent)\n\n   observation = environment!(env, chosen_action)\n\nend","category":"page"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"The only addition here is the update_parameters!(aif_agent) function, which updates the parameters of the agent, based on which priors it has been given.","category":"page"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"","category":"page"},{"location":"Simulation/","page":"Simulation","title":"Simulation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"EditURL = \"../julia_files/Fitting.jl\"","category":"page"},{"location":"Fitting/#Model-Fitting","page":"Model Fitting","title":"Model Fitting","text":"","category":"section"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"In many cases, we want to be able to draw conclusions about specific observed phenomena, such as behavioural differences between distinct populations. A conventional approach in this context is model fitting, which involves estimating the parameter values of a model (e.g., prior beliefs) that are most likely given the observed behavior of a participant. This approach is often used in fields such as computational psychiatry or mathematical psychology  to develop more precise models and theories of mental processes, to find mechanistic differences between clinical populations, or to investigate the relationship between computational constructs such as Bayesian beliefs and neuronal dynamics.","category":"page"},{"location":"Fitting/#Quick-Start","page":"Model Fitting","title":"Quick Start","text":"","category":"section"},{"location":"Fitting/#Model-Fitting-with-ActionModels.jl","page":"Model Fitting","title":"Model Fitting with ActionModels.jl","text":"","category":"section"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Model fitting in 'ActiveInference' is mediated through 'ActionModels', which is our sister package for implementing and fitting various behavioural models to data. The core of 'ActionModels' is the action model function, which takes a single observation, runs the inference scheme (updating the agent's beliefs), and calculates the probability distribution over actions from which the agent samples its actions. (Check out the ActionModels documentation for more details)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"To demonstrate this, let's define a very simple generative model with a single state factor and two possible actions, and then initialize our active inference object:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Define the number of states, observations, and controls\nn_states = [4]\nn_observations = [4]\nn_controls = [2]\n\n# Define the policy length\npolicy_length = 1\n\n# Use the create_matrix_templates function to create uniform A and B matrices.\nA, B = create_matrix_templates(n_states, n_observations, n_controls, policy_length)\n\n# Initialize an active inference object with the created matrices\naif = init_aif(A, B)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"We can now use the action_pomdp! function (which serves as our active inference \"action model\") to calculate the probability distribution over actions for a single observation:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Define observation\nobservation = [1]\n\n# Calculate action probabilities\naction_distribution = action_pomdp!(aif, observation)","category":"page"},{"location":"Fitting/#Agent-in-ActionModels.jl","page":"Model Fitting","title":"Agent in ActionModels.jl","text":"","category":"section"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Another key component of 'ActionModels' is an Agent, which wraps the action model and active inference object in a more abstract structure. The Agent is initialized using a substruct to include our active inference object, and the action model is our action_pomdp! function.","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Let's first install 'ActionModels' from the official Julia registry and import it:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Pkg.add(\"ActionModels\")\nusing ActionModels","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"    Updating registry at `~/.julia/registries/General.toml`\n   Resolving package versions...\n  No Changes to `~/work/ActiveInference.jl/ActiveInference.jl/docs/Project.toml`\n  No Changes to `~/work/ActiveInference.jl/ActiveInference.jl/docs/Manifest.toml`\n","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"We can now create an Agent with the action_pomdp! function and the active inference object:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Initialize agent with active inference object as substruct\nagent = init_agent(\n    action_pomdp!,  # The active inference action model\n    substruct = aif # The active inference object\n)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"We use an initialized Agent primarily for fitting; however, it can also be used with a set of convenience functions to run simulations, which are described in Simulation with ActionModels.","category":"page"},{"location":"Fitting/#Fitting-a-Single-Subject-Model","page":"Model Fitting","title":"Fitting a Single Subject Model","text":"","category":"section"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"We have our Agent object defined as above. Next, we need to specify priors for the parameters we want to estimate.","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"For example, let's estimate the action precision parameter α and use a Gamma distribution as its prior.","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Import the Distributions package\nusing Distributions\n\n# Define the prior distribution for the alpha parameters inside a dictionary\npriors = Dict(\"alpha\" => Gamma(1, 1))","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"We can now use the create_model function to instantiate a probabilistic model object with data. This function takes the Agent object, the priors, and a set of observations and actions as arguments.","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"First, let's define some observations and actions as vectors:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Define observations and actions\nobservations = [1, 1, 2, 3, 1, 4, 2, 1]\nactions = [2, 1, 2, 2, 2, 1, 2, 2]","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Now we can instantiate the probabilistic model object:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Create the model object\nsingle_subject_model = create_model(agent, priors, observations, actions)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"The single_subject_model can be used as a standard Turing object. Performing inference on this model is as simple as:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"results = fit_model(single_subject_model)","category":"page"},{"location":"Fitting/#Fitting-a-Model-with-Multiple-Subjects","page":"Model Fitting","title":"Fitting a Model with Multiple Subjects","text":"","category":"section"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Often, we have data from multiple subjects that we would like to fit simultaneously. The good news is that this can be done by instantiating our probabilisitc model on an entire dataset containing data from multiple subjects.","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Let's define some dataset with observations and actions for three subjects:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Import the DataFrames package\nusing DataFrames\n\n# Create a DataFrame\ndata = DataFrame(\n   subjectID = [1, 1, 1, 2, 2, 2, 3, 3, 3], # Subject IDs\n   observations = [1, 1, 2, 3, 1, 4, 2, 1, 3], # Observations\n   actions = [2, 1, 2, 2, 2, 1, 2, 2, 1] # Actions\n)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"<div><div style = \"float: left;\"><span>9×3 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">subjectID</th><th style = \"text-align: left;\">observations</th><th style = \"text-align: left;\">actions</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"To instantiate the probabilistic model on our dataset, we pass the data DataFrame to the create_model function along with the names of the columns that contain the subject identifiers, observations, and actions:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Create the model object\nmulti_subject_model = create_model(\n    agent,\n    priors,\n    data; # Dataframe\n    grouping_cols = [:subjectID], # Column with subject IDs\n    input_cols = [\"observations\"], # Column with observations\n    action_cols = [\"actions\"] # Column with actions\n)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"To fit the model, we use the fit_model function as before:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"results = fit_model(multi_subject_model)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"┌ Info: Found initial step size\n└   ϵ = 1.6\n","category":"page"},{"location":"Fitting/#Customizing-the-Fitting-Procedure","page":"Model Fitting","title":"Customizing the Fitting Procedure","text":"","category":"section"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"The fit_model function has several optional arguments that allow us to customize the fitting procedure. For example, you can specify the number of iterations, the number of chains, the sampling algorithm, or to parallelize over chains:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"results = fit_model(\n    multi_subject_model, # The model object\n    parallelization = MCMCDistributed(), # Run chains in parallel\n    sampler = NUTS(;adtype=AutoReverseDiff(compile=true), # Specify the type of sampler\n    n_itererations = 1000, # Number of iterations,\n    n_chains = 4, # Number of chains\n)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"'Turing' allows us to run distributed MCMCDistributed() or threaded MCMCThreads() parallel sampling. The default is to run chains serially MCMCSerial(). For information on the available samplers see the Turing documentation.","category":"page"},{"location":"Fitting/#Results","page":"Model Fitting","title":"Results","text":"","category":"section"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"The output of the fit_model function is an object that contains the standard 'Turing' chains which we can use to extract the summary statistics of the posterior distribution.","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Let's extract the chains from the results object:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"chains = results.chains","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Chains MCMC chain (1000×15×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 13.14 seconds\nCompute duration  = 13.14 seconds\nparameters        = parameters[1, 1], parameters[1, 2], parameters[1, 3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n        parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   ess_per_sec\n            Symbol   Float64   Float64   Float64    Float64    Float64   Float64       Float64\n\n  parameters[1, 1]    1.0300    0.9794    0.0328   633.4151   466.9619    1.0021       48.1868\n  parameters[1, 2]    1.0043    0.9398    0.0346   543.5091   442.9656    0.9997       41.3472\n  parameters[1, 3]    0.9623    0.9462    0.0315   625.7183   501.1432    1.0042       47.6012\n\nQuantiles\n        parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n            Symbol   Float64   Float64   Float64   Float64   Float64\n\n  parameters[1, 1]    0.0388    0.3211    0.7763    1.4416    3.6278\n  parameters[1, 2]    0.0248    0.2969    0.7007    1.4688    3.3773\n  parameters[1, 3]    0.0314    0.2628    0.6634    1.3709    3.6003\n","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Note that the parameter names in the chains are somewhat cryptic. We can use the rename_chains function to rename them to something more understandable:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"renamed_chains = rename_chains(chains, multi_subject_model)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"Chains MCMC chain (1000×15×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 13.14 seconds\nCompute duration  = 13.14 seconds\nparameters        = subjectID:1.alpha, subjectID:2.alpha, subjectID:3.alpha\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n         parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   ess_per_sec\n             Symbol   Float64   Float64   Float64    Float64    Float64   Float64       Float64\n\n  subjectID:1.alpha    1.0300    0.9794    0.0328   633.4151   466.9619    1.0021       48.1868\n  subjectID:2.alpha    1.0043    0.9398    0.0346   543.5091   442.9656    0.9997       41.3472\n  subjectID:3.alpha    0.9623    0.9462    0.0315   625.7183   501.1432    1.0042       47.6012\n\nQuantiles\n         parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n             Symbol   Float64   Float64   Float64   Float64   Float64\n\n  subjectID:1.alpha    0.0388    0.3211    0.7763    1.4416    3.6278\n  subjectID:2.alpha    0.0248    0.2969    0.7007    1.4688    3.3773\n  subjectID:3.alpha    0.0314    0.2628    0.6634    1.3709    3.6003\n","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"That looks better! We can now use the 'StatsPlots' package to plot the chain traces and density plots of the posterior distributions for all subjects:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"using StatsPlots # Load the StatsPlots package\n\nplot(renamed_chains)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"(Image: image2)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"We can also visualize the posterior distributions against the priors. This can be done by first taking samples from the prior:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"# Sample from the prior\nprior_chains = sample(multi_subject_model, Prior(), 1000)\n# Rename parameters in the prior chains\nrenamed_prior_chains = rename_chains(prior_chains, multi_subject_model)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"To plot the posterior distributions against the priors, we use the plot_parameters function:","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"plot_parameters(renamed_prior_chains, renamed_chains)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"(Image: image3)","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"","category":"page"},{"location":"Fitting/","page":"Model Fitting","title":"Model Fitting","text":"This page was generated using Literate.jl.","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"EditURL = \"../julia_files/GenerativeModelCreation.jl\"","category":"page"},{"location":"GenerativeModelCreation/#Creating-the-POMDP-Generative-Model","page":"Creation of the Generative Model","title":"Creating the POMDP Generative Model","text":"","category":"section"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"In this section we will go through the process of creating a generative model and how it should be structured. In this part, we will show the code necessary for correct typing of the generative model. For a theoretical explanation of POMDPs look under the \"Theory\" section further down in the documentation.","category":"page"},{"location":"GenerativeModelCreation/#Typing-of-the-POMDP-parameters","page":"Creation of the Generative Model","title":"Typing of the POMDP parameters","text":"","category":"section"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"In ActiveInference.jl, it is important that the parameters describing the generative model is typed correctly. The correct typing of the generative model parameters, which often take the shapes of matrices, tensors and vectors. The collections of generative model parameters are colloquially referred to as A, B, C, D, and E. We will denote these parameters by their letter in bold. For a quick refresher this is the vernacular used to describe these parameter collections:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"A - Observation Likelihood Model\nB - Transition Likelihood Model\nC - Prior over Observations\nD - Prior over States\nE - Prior over Policies","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"These should be typed the following way in ActiveInference.jl:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"A = Vector{Array{Float64, 3}}(undef, n_modalities)\nB = Vector{Array{Float64, 3}}(undef, n_factors)\nC = Vector{Vector{Float64, 3}}(undef, n_modalities)\nD = Vector{Vector{Float64, 3}}(undef, n_factors)\nD = Vector{Float64, 3}(undef, n_policies)","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"Each of the parameter collections are vectors, where each index in the vector contains the parameters associated with a specific modality or factor. However, creating these from scratch is not necessary, as we have created a helper function that can create a template for these parameters.","category":"page"},{"location":"GenerativeModelCreation/#Helper-Function-for-GM-Templates","page":"Creation of the Generative Model","title":"Helper Function for GM Templates","text":"","category":"section"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"Luckily, there is a helper function that helps create templates for the generative model parameters. This function is called create_matrix_templates.","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"A, B, C, D, E = create_matrix_templates(n_states::Vector{Int64}, n_observations::Vector{Int64}, n_controls::Vector{Int64}, policy_length::Int64, template_type::String)","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"This function takes the five arguments n_states, n_observations, n_controls, policy_length, and template_type, which have all the necessary information to create the right structure of the generative model parameters. We will go through these arguments one by one:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"\n","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"n_states - This is the number of states in the environment. The environment can have different kinds of states, which are often referred to as factors. Could be a location factor and a reward condition factor. It takes a vector of integers, where each integer represents a factor, and the value of the integer is the number of states in that factor. E.g. if we had an environment with two factors, one location factor with 4 states and one reward condition factor with 2 states, the argument would look like this: [4,2]","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"\n","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"n_observations - This is the number of observations the agent can make in the environment. The observations are often referred to as modalities. Could be a location modality, a reward modality and a cue modality. Similarly to the first argument, it takes a vector of integers, where each integer represents a modality, and the value of the integer is the number of observations in that modality. E.g. if we had an environment with three modalities, one location modality with 4 observations, one reward modality with 3 observations and one cue modality with 2 observations, the argument would look like this: [4,3,2]","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"\n","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"n_controls - This is the number of controls the agent have in the environment. The controls are the actions the agent can take in the different factors. Could be moving left or right, or choosing between two different rewards. It has one control integer for each factor, where the integer represents the number of actions in that factor. If the agent cannot control a factor, the integer should be 1. E.g. if we had an environment with two factors, one location factor with 4 actions and one reward condition factor with 1 action, the argument would look like this: [4,1]","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"\n","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"policy_length - This is the length of the policies of the agent, and is taken as an integer. The policy is a sequence of actions the agent can take in the environment. The length of the policy describes how many actions into the future the agent is planning. For example, if the agent is planning two steps into the future, the policy length would be 2, and each policy would consist of 2 actions. In that case the argument would look like this: 2","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"\n","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"template_type - This is a string that describes the type of template you want to create, or in other words, the initial filling of the generative model structure. There are three options; \"uniform\", which is default, \"random\", and \"zeros\".","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"If we were to use the arguments from the examples above, the function call would look like this:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"n_states = [4,2]\nn_observations = [4,3,2]\nn_controls = [4,1]\npolicy_length = 2\ntemplate_type = \"zeros\"\n\nA, B, C, D, E = create_matrix_templates(n_states, n_observations, n_controls, policy_length, template_type);","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"When these parameter collections have been made, each factor/modality can be accessed by indexing the collection with the factor/modality index like:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"A[1] # Accesses the first modality in the observation likelihood model\nB[2] # Accesses the second factor in the transition likelihood model\nC[3] # Accesses the third modality in the prior over observations\nD[1] # Accesses the first factor in the prior over states","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"The E-parameters are not a divided into modalities or factors, as they are the prior over policies.","category":"page"},{"location":"GenerativeModelCreation/#Populating-the-Parameters","page":"Creation of the Generative Model","title":"Populating the Parameters","text":"","category":"section"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"Now that the generative model parameter templates ahave been created, they can now be filled with the desired values, ie. populating the parameters. Let's take the example of filling A with some valus. To start, let's print out the first modality of the A so we get a sense of the dimensions:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"A[1]","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"4×4×2 Array{Float64, 3}:\n[:, :, 1] =\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\n[:, :, 2] =\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"For a quick recap on the POMDP generative model parameteres look up the POMDP Theory section further down in the documentation.","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"For now, we'll suffice to say that the first modality of A is a 3D tensor, where the first dimension are observations in the first modality, the second dimension the first factor, and the third dimension is the second factor. Remember A maps the agents beliefs on how states generate observations. In this case, we have two 4x4 matrices, one matrix for each state int the second factor. This could be how location observations (1st dimenstion) map onto location states (2nd dimension) and reward condition (3rd dimension). For the sake of simplicity, let's assume that the agent can infer location states with certainty based on location observations. In this case we could populate the first modality of A like this:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"# For reward condition right\nA[1][:,:,1] = [ 1.0  0.0  0.0  0.0\n                0.0  1.0  0.0  0.0\n                0.0  0.0  1.0  0.0\n                0.0  0.0  0.0  1.0 ]\n\n# For reward condition left\nA[1][:,:,2] = [ 1.0  0.0  0.0  0.0\n                0.0  1.0  0.0  0.0\n                0.0  0.0  1.0  0.0\n                0.0  0.0  0.0  1.0 ]","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"In this case the agent would infer the location state with certainty based on the location observations. One could also make the A more noisy in this modality, which could look like:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"# For reward condition right\nA[1][:,:,1] = [ 0.7  0.1  0.1  0.1\n                0.1  0.7  0.1  0.1\n                0.1  0.1  0.7  0.1\n                0.1  0.1  0.1  0.7 ]\n\n# For reward condition left\nA[1][:,:,2] = [ 0.7  0.1  0.1  0.1\n                0.1  0.7  0.1  0.1\n                0.1  0.1  0.7  0.1\n                0.1  0.1  0.1  0.7 ]","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"Importantly the columns should always add up to 1, as we are here dealing with categorical probability distributions. For the other parameters, the process is similar, but the dimensions of the matrices will differ. For B the dimensions are states to states, and for C and D the dimensions are states to observations and states to factors respectively. Look up the T-Maze Simulation (insert reference here) example for a full example of how to populate the generative model parameters.","category":"page"},{"location":"GenerativeModelCreation/#Creating-Dirichlet-Priors-over-Parameters","page":"Creation of the Generative Model","title":"Creating Dirichlet Priors over Parameters","text":"","category":"section"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"When learning is included, we create Dirichlet priors over the parameters A, B, and D. We usually do this by taking the created A, B, and D parameters and multiplying them with a scalar, which is the concentration parameter of the Dirichlet distribution. For more information on the specifics of learning and Dirichlet priors, look under the Active Inference Theory section in the documentation. Note here, that when we implement learning of a parameter, the parameter is going to be defined by its prior and no longer the initial parameter that we specified. This is because the agent will update the parameter based on the prior and the data it receives. An example of how we would create a Dirichlet prior over A could look:","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"pA = deepcopy(A)\nscale_concentration_parameter = 2.0\npA .*= scale_concentration_parameter","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"This is not relevant if learning is not included. If learning is not included, the parameters are fixed and the agent will not update them. The value of the scaling parameter determines how much each data observation impacts the update of the parameter. If the scaling is high, e.g. 50, then adding one data point will have a small impact on the parameter. If the scaling is low, e.g. 0.1, then adding one data point will have a large impact on the parameter. The update function updates the parameters by normalising the concentration parameters of the Dirichlet distribution.","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"","category":"page"},{"location":"GenerativeModelCreation/","page":"Creation of the Generative Model","title":"Creation of the Generative Model","text":"This page was generated using Literate.jl.","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"EditURL = \"../julia_files/GenerativeModelTheory.jl\"","category":"page"},{"location":"GenerativeModelTheory/#The-Generative-Model-Conceptually","page":"POMDP Theory","title":"The Generative Model Conceptually","text":"","category":"section"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"The generative model is the parameters that constitute the agent's beliefs on how the hidden states of the environment generates observations based on states, and how hidden underlying states changes over time. In the generative model is also the beliefs of how the agent through actions can influence the states of the environment. Together this holds the buidling blocks that allows for the perception-action loop.","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"There are five main buidling blocks of the generative model which are; A, B, C, D, and E. Each of these contain parameters that describe the agent's beliefs about the environment. We will now go through these conecptually one at a time.","category":"page"},{"location":"GenerativeModelTheory/#A","page":"POMDP Theory","title":"A","text":"","category":"section"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"A is the observation likelihood model, and describes the agent's beliefs about how the hidden states of the environment generates observations. Practically in this package, and other POMDP implemantations as well, this is described through a series of categorical distributions, meaning that for each observation, there is a categorical probability distribution over how likely each hidden state is to generate that observation. Let us for example imagine a simple case, where the agent is in a four location state environment, could be a 2x2 gridworld. In this case, there would be one obseration linked to each hidden state, and A then maps the agent's belief of how likely each hidden location state is to generate each observation. The agent can then use this belief to infer what state it is in based on the observation it receives. Let's look at an example A, which in this case would be a 4x4 matrix:","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"A =\noversettextnormalsize Statesvphantombeginarrayc 0  0 endarray\n    beginarraycccc\n        1  0  0  0 \n        0  1  0  0 \n        0  0  1  0 \n        0  0  0  1\n    endarray\n\nquad\ntextnormalsize Observations","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"In this case, the agent is quite certain about which states produces which observations. This matrix could be made more uncertain to the point of complete uniformity and it could be made certain in the sense of each column being a one-hot vector. In the case of a certain A, the generative model stops being a \"partially observable\" Markov decision process, and becomes a fully observable one, making it a Markov decision process (MDP). For a more technical and mathematical definition of the observation likelihood model.","category":"page"},{"location":"GenerativeModelTheory/#B","page":"POMDP Theory","title":"B","text":"","category":"section"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"B is the transition likelihood model that encodes the agent's beliefs about how the hidden states of the environment changes over time. This is also made up of categorical distributions, though instead of observations to states, it maps states to states. If we take the same case again, a 2x2 gridworld, we would have a 4x4 matrix that describes how the agent believes the states evolve over time. An extra addition to B, is that it can depend on actions, meaning that it can believe that the hidden states of the environment change differently depending on the action taken by the agent. Due to this fact, we would the have a matrix for each action, making B a 3 dimensional tensor, with 2 dimensions for the \"from\" state and the \"to\" state, and then an action dimension. Let's look at an example of a slice of B for the action \"down\" in the grid world, which in this case would be a 4x4 matrix:","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"B(down) =\noversettextnormalsize Previous Statevphantombeginarrayc 0  0 endarray\n    beginarraycccc\n        0  0  0  0 \n        1  1  0  0 \n        0  0  0  0 \n        0  0  1  1\n    endarray\n\nquad\ntextnormalsize Current State","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"We could make 3 more similar matrices for the actions \"up\", \"left\", and \"right\", and then we would have the full B tensor for the gridworld. But here, the main point is that B decsribes the agent's belief of how hidden states change over time, and this can be dependent on actions, but might also be independent of actions, and thus the agent believes that the changes are out of its control.","category":"page"},{"location":"GenerativeModelTheory/#C","page":"POMDP Theory","title":"C","text":"","category":"section"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"C is the prior over observations, also called preferences over observations. This is an integral part of the utility of certain observations, i.e. it encodes how much the agent prefers or dislikes certain observations. C is a simple vector over observations, where each entry is a value that describes the utility or preference of that specific observation. If we continue with the simple 2x2 gridworld example, we would have 4 observations, one for each location state (same amount of observations as in A). Let's say that we would like for the agent to dislike observing the top left location (indexed as 1), and prefer the bottom right location (indexed as 4). We would then create C in the following way:","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"C =\nbeginarraycccc\n    -2  0  0  2 \nendarray","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"The magnitude of the values in C is arbitrary, and denotes a ratio and amount of dislike/preference. Here, we have chosen the value of -2 and 2 to encode that the agent dislikes the top left location just as much as it likes the bottom right location. The zeros in between just means that the agent has not preference or dislike for these locatin observations. Note that since C is not a categorical distribution, it does not need to sum to 1, and the values can be any real number.","category":"page"},{"location":"GenerativeModelTheory/#D","page":"POMDP Theory","title":"D","text":"","category":"section"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"D is the prior over states, and is the agent's beliefs about the initial state of the environment. This is also a simple vector that is a categorical distribution. Note that if A is certain, then D does not matter a lot for the inference process, as the agent can infer the state from the observation. However, if A is uncertain, then D becomes very important, as it serves as the agent's anchor point of where it is initially in the environment. In the case of out 2x2 gridworld, we would have a vector with 4 entries, one for each location state. If we assume that the agent correctly infers it's initial location as upper left corner, D would look like:","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"D =\nbeginarraycccc\n    1  0  0  0 \nendarray","category":"page"},{"location":"GenerativeModelTheory/#E","page":"POMDP Theory","title":"E","text":"","category":"section"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"E is the prior over policies, and can be described as the agent's habits. Policies in Active Inference vernacular are sets of actions, with an action for each step in the future, specified by a policy length. It is a categorical distribution over policies, with a probability for each policy. This will have an effect on the agent posterior over policies, which is the probability of taking a certain action at a time step. This will often be set to a uniform distribution, if we are not interested in giving the agent habits. Let us assume that we will give our agent a uniform E for a policy length of 2, this mean that we will have a uniform categorical distribution over 16 possible policies (4 (actions) ^ 2 (policy length)):","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"E =\nbeginarraycccc\n00625  00625  00625  00625  00625  00625  00625  00625  00625  00625  00625  00625  00625  00625  00625  00625 \nendarray","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"","category":"page"},{"location":"GenerativeModelTheory/","page":"POMDP Theory","title":"POMDP Theory","text":"This page was generated using Literate.jl.","category":"page"},{"location":"SimulationActionModels/","page":"Simulation with ActionModels.jl","title":"Simulation with ActionModels.jl","text":"EditURL = \"../julia_files/SimulationActionModels.jl\"","category":"page"},{"location":"SimulationActionModels/#Simulation-with-ActionModels.jl","page":"Simulation with ActionModels.jl","title":"Simulation with ActionModels.jl","text":"","category":"section"},{"location":"SimulationActionModels/","page":"Simulation with ActionModels.jl","title":"Simulation with ActionModels.jl","text":"","category":"page"},{"location":"SimulationActionModels/","page":"Simulation with ActionModels.jl","title":"Simulation with ActionModels.jl","text":"This page was generated using Literate.jl.","category":"page"},{"location":"WhyActiveInference/","page":"Why Work with Active Inference?","title":"Why Work with Active Inference?","text":"EditURL = \"../julia_files/WhyActiveInference.jl\"","category":"page"},{"location":"WhyActiveInference/#Why-Work-with-Active-Inference?","page":"Why Work with Active Inference?","title":"Why Work with Active Inference?","text":"","category":"section"},{"location":"WhyActiveInference/","page":"Why Work with Active Inference?","title":"Why Work with Active Inference?","text":"Pros Cons\nEasy to use Limited features\nWidely supported Not fully customizable\nLightweight Lacks some advanced formatting","category":"page"},{"location":"WhyActiveInference/","page":"Why Work with Active Inference?","title":"Why Work with Active Inference?","text":"","category":"page"},{"location":"WhyActiveInference/","page":"Why Work with Active Inference?","title":"Why Work with Active Inference?","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"EditURL = \"../julia_files/Introduction.jl\"","category":"page"},{"location":"Introduction/#Introduction-to-the-ActiveInference.jl-package","page":"Introduction","title":"Introduction to the ActiveInference.jl package","text":"","category":"section"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"This package is a Julia implementation of the Active Inference framework, with a specific focus on cognitive modelling. In its current implementation, the package is designed to handle scenarios that can be modelled as discrete state spaces, with 'partially observable Markov decision process' (POMDP). In this documentation we will go through the basic concepts of how to use the package for different purposes; simulation and model inversion with Active Inference, also known as parameter estimation.","category":"page"},{"location":"Introduction/#Installing-Package","page":"Introduction","title":"Installing Package","text":"","category":"section"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"Installing the package is done by adding the package from the julia official package registry in the following way:","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"using Pkg\nPkg.add(\"ActiveInference\")","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"Now, having added the package, we simply import the package to start using it:","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"using ActiveInference","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"In the next section we will go over the basic concepts of how to start using the package. We do this by providing instructions on how to create and design a generative model, that can be used for both simulation and parameter estimation.","category":"page"},{"location":"Introduction/#Workflows","page":"Introduction","title":"Workflows","text":"","category":"section"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"This package has two main functions that can be used in a variety of workflows; simulation and model fitting. We will here outline two different kind of workflows that can be implemented using the ActiveInference.jl package. The first one will be a simulation workflow, where we are interested in simulating the agent's behaviour in a given environment. Here, we might be interested in the behevaiour of a simulated active inference agent in an environment, given some specified parameters. The second is a model fitting workflow, which is interesting for people in computational psychiatry/mathematical psychology. Here, we use observed data to fit an active inference mode and we will use a classical bayesian workflow in this regard. See Bayesian Workflow for Generative Modeling in Computational Psychiatry","category":"page"},{"location":"Introduction/#Simulation","page":"Introduction","title":"Simulation","text":"","category":"section"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"In the simulation workflow, we are interested in simulating the agent's behaviour in a given environment. We might have some question wrt. behaviour expected under active inference, or we want to figure out whether our experimental task is suitable for active inference modelling. For these purposes, we will use a simple simulation workflow:","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"Decide on an environment the agent will interact with\nCreate a generative model based on that environment\nSimulate the agent's behaviour in that environment\nAnalyse and visualize the agent's behaviour and inferences\nPotential parameter recovery by model fitting on observed data","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"First, deciding on the environment entails that we have some dynamic that we are interested in from an active inference perspective - a specific research question. Classical examples of environments are T-Mazes and Multi-Armed Bandits, that often involves some decision-making, explore-exploit and information seeking dynamics. These environments are easy to encode as POMDPs and are therefore suitable for active inference modelling. Importantly though this can be any kind of environment that provides the active inference agent with observations, and most often will also take actions so that the agent can interact with the environment.","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"Based on an environment, you then create the generative model of the agent. Look under the Creating the POMDP Generative Model section for more information on how to do this.","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"You then simulate the agent's behaviour in that environment through a perception-action-learning loop, as described under the 'Simulation' section. After this, you can analyse and visualize the agent's behaviour and inferences, and investigate what was important to the research question you had in mind.","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"Parameter recovery is also a possibility here, if you are interested in seeing whether the parameters you are interested in are in fact recoverable, or there is a dynamic in the agent-environment interaction, where a parameter cannot be specified but only inferred. For an example of the latter, look up the 'As One and Many: Relating Individual and Emergent Group-Level Generative Models in Active Inference' paper, where parameters are inferred from group-level behaviour.","category":"page"},{"location":"Introduction/#Model-Fitting-with-observed-data","page":"Introduction","title":"Model Fitting with observed data","text":"","category":"section"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"For","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"Introduction/","page":"Introduction","title":"Introduction","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Index","title":"Index","text":"CurrentModule = ActiveInference","category":"page"},{"location":"#ActiveInference","page":"Index","title":"ActiveInference","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Documentation for ActiveInference.","category":"page"},{"location":"","page":"Index","title":"Index","text":"","category":"page"},{"location":"","page":"Index","title":"Index","text":"Modules = [ActiveInference, ActiveInference.Environments]","category":"page"},{"location":"#ActiveInference.action_select-Tuple{Any}","page":"Index","title":"ActiveInference.action_select","text":"Selects action from computed actions probabilities – used for stochastic action sampling \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.array_of_any_zeros-Tuple{Any}","page":"Index","title":"ActiveInference.array_of_any_zeros","text":"Creates an array of \"Any\" with the desired number of sub-arrays filled with zeros\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.bayesian_model_average-Tuple{Any, Any}","page":"Index","title":"ActiveInference.bayesian_model_average","text":"Calculate Bayesian Model Average (BMA)\n\nCalculates the Bayesian Model Average (BMA) which is used for the State Action Prediction Error (SAPE). It is a weighted average of the expected states for all policies weighted by the posterior over policies. The qs_pi_all should be the collection of expected states given all policies. Can be retrieved with the get_expected_states function.\n\nqs_pi_all: Vector{Any} \n\nq_pi: Vector{Float64}\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.calc_expected_utility-Tuple{Any, Any}","page":"Index","title":"ActiveInference.calc_expected_utility","text":"Calculate Expected Utility \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.calc_free_energy","page":"Index","title":"ActiveInference.calc_free_energy","text":"Calculate Free Energy \n\n\n\n\n\n","category":"function"},{"location":"#ActiveInference.calc_pA_info_gain-Tuple{Any, Any, Any}","page":"Index","title":"ActiveInference.calc_pA_info_gain","text":"Calculate observation to state info Gain \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.calc_pB_info_gain-NTuple{4, Any}","page":"Index","title":"ActiveInference.calc_pB_info_gain","text":"Calculate state to state info Gain \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.calc_states_info_gain-Tuple{Any, Any}","page":"Index","title":"ActiveInference.calc_states_info_gain","text":"Calculate States Information Gain \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.calculate_SAPE-Tuple{ActiveInference.AIF}","page":"Index","title":"ActiveInference.calculate_SAPE","text":"Calculate State-Action Prediction Error \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.calculate_bayesian_surprise-Tuple{Any, Any}","page":"Index","title":"ActiveInference.calculate_bayesian_surprise","text":"Calculate Bayesian Surprise \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.capped_log-Tuple{Array{Float64}}","page":"Index","title":"ActiveInference.capped_log","text":"capped_log(array::Array{Float64})\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.capped_log-Tuple{Real}","page":"Index","title":"ActiveInference.capped_log","text":"capped_log(x::Real)\n\nArguments\n\nx::Real: A real number.\n\nReturn the natural logarithm of x, capped at the machine epsilon value of x.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.capped_log-Tuple{Vector{Real}}","page":"Index","title":"ActiveInference.capped_log","text":"capped_log(array::Vector{Real})\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.capped_log-Union{Tuple{Array{T}}, Tuple{T}} where T<:Real","page":"Index","title":"ActiveInference.capped_log","text":"capped_log(array::Array{T}) where T <: Real\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.capped_log_array-Tuple{Any}","page":"Index","title":"ActiveInference.capped_log_array","text":"Apply capped_log to array of arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.check_probability_distribution-Union{Tuple{Array{Vector{T}, 1}}, Tuple{T}} where T<:Real","page":"Index","title":"ActiveInference.check_probability_distribution","text":"Check if the vector of vectors is a proper probability distribution.\n\nArguments\n\n(Array::Vector{Vector{T}}) where T<:Real\n\nThrows an error if the array is not a valid probability distribution:\n\nThe values must be non-negative.\nThe sum of the values must be approximately 1.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.check_probability_distribution-Union{Tuple{Vector{<:Array{T}}}, Tuple{T}} where T<:Real","page":"Index","title":"ActiveInference.check_probability_distribution","text":"Check if the vector of arrays is a proper probability distribution.\n\nArguments\n\n(Array::Vector{<:Array{T}}) where T<:Real\n\nThrows an error if the array is not a valid probability distribution:\n\nThe values must be non-negative.\nThe sum of the values must be approximately 1.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.check_probability_distribution-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"Index","title":"ActiveInference.check_probability_distribution","text":"Check if the vector is a proper probability distribution.\n\nArguments\n\n(Vector::Vector{T}) where T<:Real : The vector to be checked.\n\nThrows an error if the array is not a valid probability distribution:\n\nThe values must be non-negative.\nThe sum of the values must be approximately 1.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.compute_accuracy-Tuple{Any, Array{Vector{T}, 1} where T<:Real}","page":"Index","title":"ActiveInference.compute_accuracy","text":"Calculate Accuracy Term \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.compute_accuracy_new-Tuple{Any, Vector{Vector{Real}}}","page":"Index","title":"ActiveInference.compute_accuracy_new","text":"Edited Compute Accuracy [Still needs to be nested within Fixed-Point Iteration] \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.construct_policies-Tuple{Vector{T} where T<:Real}","page":"Index","title":"ActiveInference.construct_policies","text":"construct_policies(n_states::Vector{T} where T <: Real; n_controls::Union{Vector{T}, Nothing} where T <: Real=nothing, \n                   policy_length::Int=1, controllable_factors_indices::Union{Vector{Int}, Nothing}=nothing)\n\nConstruct policies based on the number of states, controls, policy length, and indices of controllable state factors.\n\nArguments\n\nn_states::Vector{T} where T <: Real: A vector containing the number of  states for each factor.\nn_controls::Union{Vector{T}, Nothing} where T <: Real=nothing: A vector specifying the number of allowable actions for each state factor. \npolicy_length::Int=1: The length of policies. (planning horizon)\ncontrollable_factors_indices::Union{Vector{Int}, Nothing}=nothing: A vector of indices identifying which state factors are controllable.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.create_matrix_templates","page":"Index","title":"ActiveInference.create_matrix_templates","text":"create_matrix_templates(shapes::Vector{Int64}, template_type::String)\n\nCreates templates based on the specified shapes vector and template type. Templates can be uniform, random, or filled with zeros.\n\nArguments\n\nshapes::Vector{Int64}: A vector specifying the dimensions of each template to create.\ntemplate_type::String: The type of templates to create. Can be \"uniform\" (default), \"random\", or \"zeros\".\n\nReturns\n\nA vector of arrays, each corresponding to the shape given by the input vector.\n\n\n\n\n\n","category":"function"},{"location":"#ActiveInference.create_matrix_templates-Tuple{Vector{Int64}, Vector{Int64}, Vector{Int64}, Int64}","page":"Index","title":"ActiveInference.create_matrix_templates","text":"create_matrix_templates(n_states::Vector{Int64}, n_observations::Vector{Int64}, n_controls::Vector{Int64}, policy_length::Int64, template_type::String = \"uniform\")\n\nCreates templates for the A, B, C, D, and E matrices based on the specified parameters.\n\nArguments\n\nn_states::Vector{Int64}: A vector specifying the dimensions and number of states.\nn_observations::Vector{Int64}: A vector specifying the dimensions and number of observations.\nn_controls::Vector{Int64}: A vector specifying the number of controls per factor.\npolicy_length::Int64: The length of the policy sequence. \ntemplate_type::String: The type of templates to create. Can be \"uniform\", \"random\", or \"zeros\". Defaults to \"uniform\".\n\nReturns\n\nA, B, C, D, E: The generative model as matrices and vectors.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.create_matrix_templates-Tuple{Vector{Int64}}","page":"Index","title":"ActiveInference.create_matrix_templates","text":"create_matrix_templates(shapes::Vector{Int64})\n\nCreates uniform templates based on the specified shapes vector.\n\nArguments\n\nshapes::Vector{Int64}: A vector specifying the dimensions of each template to create.\n\nReturns\n\nA vector of normalized arrays.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.create_matrix_templates-Tuple{Vector{Vector{Int64}}, String}","page":"Index","title":"ActiveInference.create_matrix_templates","text":"create_matrix_templates(shapes::Vector{Vector{Int64}}, template_type::String)\n\nCreates a multidimensional template based on the specified vector of shape vectors and template type. Templates can be uniform, random, or filled with zeros.\n\nArguments\n\nshapes::Vector{Vector{Int64}}: A vector of vectors, where each vector represent a dimension of the template to create.\ntemplate_type::String: The type of templates to create. Can be \"uniform\" (default), \"random\", or \"zeros\".\n\nReturns\n\nA vector of arrays, each having the multi-dimensional shape specified in the input vector.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.create_matrix_templates-Tuple{Vector{Vector{Int64}}}","page":"Index","title":"ActiveInference.create_matrix_templates","text":"create_matrix_templates(shapes::Vector{Vector{Int64}})\n\nCreates a uniform, multidimensional template based on the specified shapes vector.\n\nArguments\n\nshapes::Vector{Vector{Int64}}: A vector of vectors, where each vector represent a dimension of the template to create.\n\nReturns\n\nA vector of normalized arrays (uniform distributions), each having the multi-dimensional shape specified in the input vector.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.dot_likelihood-Tuple{Any, Any}","page":"Index","title":"ActiveInference.dot_likelihood","text":"Dot-Product Function \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.fixed_point_iteration-Tuple{Array{Array{T, N}, 1} where {T<:Real, N}, Vector{Vector{Float64}}, Vector{Int64}, Vector{Int64}}","page":"Index","title":"ActiveInference.fixed_point_iteration","text":"Run State Inference via Fixed-Point Iteration \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.get_expected_obs-Tuple{Any, Array{Array{T, N}, 1} where {T<:Real, N}}","page":"Index","title":"ActiveInference.get_expected_obs","text":"Get Expected Observations \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.get_expected_states-Tuple{Array{Vector{T}, 1} where T<:Real, Any, Matrix{Int64}}","page":"Index","title":"ActiveInference.get_expected_states","text":"Get Expected States \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.get_expected_states-Tuple{Vector{Vector{Float64}}, Any, Vector{Matrix{Int64}}}","page":"Index","title":"ActiveInference.get_expected_states","text":"Multiple dispatch for all expected states given all policies\n\nMultiple dispatch for getting expected states for all policies based on the agents currently inferred states and the transition matrices for each factor and action in the policy.\n\nqs::Vector{Vector{Real}} \n\nB: Vector{Array{<:Real}} \n\npolicy: Vector{Matrix{Int64}}\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.get_joint_likelihood-Tuple{Any, Any, Any}","page":"Index","title":"ActiveInference.get_joint_likelihood","text":"Get Joint Likelihood \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.get_log_action_marginals-Tuple{Any}","page":"Index","title":"ActiveInference.get_log_action_marginals","text":"Function to get log marginal probabilities of actions \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.get_model_dimensions","page":"Index","title":"ActiveInference.get_model_dimensions","text":"Get Model Dimensions from either A or B Matrix \n\n\n\n\n\n","category":"function"},{"location":"#ActiveInference.infer_policies!-Tuple{ActiveInference.AIF}","page":"Index","title":"ActiveInference.infer_policies!","text":"Update the agents's beliefs over policies \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.infer_states!-Tuple{ActiveInference.AIF, Vector{Int64}}","page":"Index","title":"ActiveInference.infer_states!","text":"Update the agents's beliefs over states \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.init_aif-Tuple{Any, Any}","page":"Index","title":"ActiveInference.init_aif","text":"Initialize Active Inference Agent function initaif(         A,         B;         C=nothing,         D=nothing,         E = nothing,         pA = nothing,         pB = nothing,          pD = nothing,         parameters::Union{Nothing, Dict{String,Real}} = nothing,         settings::Union{Nothing, Dict} = nothing,         savehistory::Bool = true)\n\nArguments\n\n'A': Relationship between hidden states and observations.\n'B': Transition probabilities.\n'C = nothing': Prior preferences over observations.\n'D = nothing': Prior over initial hidden states.\n'E = nothing': Prior over policies. (habits)\n'pA = nothing':\n'pB = nothing':\n'pD = nothing':\n'parameters::Union{Nothing, Dict{String,Real}} = nothing':\n'settings::Union{Nothing, Dict} = nothing':\n'settings::Union{Nothing, Dict} = nothing':\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.kl_divergence-Tuple{Vector{Vector{Vector{Real}}}, Vector{Vector{Vector{Real}}}}","page":"Index","title":"ActiveInference.kl_divergence","text":"kl_divergence(P::Vector{Vector{Vector{Float64}}}, Q::Vector{Vector{Vector{Float64}}})\n\nArguments\n\nP::Vector{Vector{Vector{Real}}}\nQ::Vector{Vector{Vector{Real}}}\n\nReturn the Kullback-Leibler (KL) divergence between two probability distributions.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.normalize_arrays-Tuple{Vector{<:Array{<:Real}}}","page":"Index","title":"ActiveInference.normalize_arrays","text":"Normalizes multiple arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.normalize_arrays-Tuple{Vector{Any}}","page":"Index","title":"ActiveInference.normalize_arrays","text":"Normalizes multiple arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.normalize_distribution-Tuple{Any}","page":"Index","title":"ActiveInference.normalize_distribution","text":"Normalizes a Categorical probability distribution\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.onehot-Tuple{Int64, Int64}","page":"Index","title":"ActiveInference.onehot","text":"Creates a onehot encoded vector \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.outer_product","page":"Index","title":"ActiveInference.outer_product","text":"Multi-dimensional outer product \n\n\n\n\n\n","category":"function"},{"location":"#ActiveInference.process_observation-Tuple{Int64, Int64, Vector{Int64}}","page":"Index","title":"ActiveInference.process_observation","text":"process_observation(observation::Int, n_modalities::Int, n_observations::Vector{Int})\n\nProcess a single modality observation. Returns a one-hot encoded vector. \n\nArguments\n\nobservation::Int: The index of the observed state with a single observation modality.\nn_modalities::Int: The number of observation modalities in the observation. \nn_observations::Vector{Int}: A vector containing the number of observations for each modality.\n\nReturns\n\nVector{Vector{Real}}: A vector containing a single one-hot encoded observation.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.process_observation-Tuple{Union{Tuple{Vararg{Int64}}, Array{Int64}}, Int64, Vector{Int64}}","page":"Index","title":"ActiveInference.process_observation","text":"process_observation(observation::Union{Array{Int}, Tuple{Vararg{Int}}}, n_modalities::Int, n_observations::Vector{Int})\n\nProcess observation with multiple modalities and return them in a one-hot encoded format \n\nArguments\n\nobservation::Union{Array{Int}, Tuple{Vararg{Int}}}: A collection of indices of the observed states for each modality.\nn_modalities::Int: The number of observation modalities in the observation. \nn_observations::Vector{Int}: A vector containing the number of observations for each modality.\n\nReturns\n\nVector{Vector{Real}}: A vector containing one-hot encoded vectors for each modality.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.sample_action!-Tuple{ActiveInference.AIF}","page":"Index","title":"ActiveInference.sample_action!","text":"Sample action from the beliefs over policies \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.sample_action-Tuple{Any, Vector{Matrix{Int64}}, Any}","page":"Index","title":"ActiveInference.sample_action","text":"Sample Action [Stochastic or Deterministic] \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.select_highest-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"Index","title":"ActiveInference.select_highest","text":"Selects the highest value from Array – used for deterministic action sampling \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.softmax_array-Tuple{Any}","page":"Index","title":"ActiveInference.softmax_array","text":"Softmax Function for array of arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.spm_wnorm-Tuple{Any}","page":"Index","title":"ActiveInference.spm_wnorm","text":"SPM_wnorm \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.update_A!-Tuple{ActiveInference.AIF}","page":"Index","title":"ActiveInference.update_A!","text":"Update A-matrix \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.update_B!-Tuple{ActiveInference.AIF}","page":"Index","title":"ActiveInference.update_B!","text":"Update B-matrix \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.update_D!-Tuple{ActiveInference.AIF}","page":"Index","title":"ActiveInference.update_D!","text":"Update D-matrix \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.update_obs_likelihood_dirichlet-NTuple{4, Any}","page":"Index","title":"ActiveInference.update_obs_likelihood_dirichlet","text":"Update obs likelihood matrix \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.update_posterior_policies","page":"Index","title":"ActiveInference.update_posterior_policies","text":"Update Posterior over Policies \n\n\n\n\n\n","category":"function"},{"location":"#ActiveInference.update_posterior_states-Tuple{Array{Array{T, N}, 1} where {T<:Real, N}, Vector{Int64}}","page":"Index","title":"ActiveInference.update_posterior_states","text":"Update Posterior States \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.update_state_likelihood_dirichlet-Tuple{Any, Any, Any, Array{Vector{T}, 1} where T<:Real, Any}","page":"Index","title":"ActiveInference.update_state_likelihood_dirichlet","text":"Update state likelihood matrix \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.update_state_prior_dirichlet-Tuple{Any, Array{Vector{T}, 1} where T<:Real}","page":"Index","title":"ActiveInference.update_state_prior_dirichlet","text":"Update prior D matrix \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.bayesian_model_average-Tuple{Any, Any}","page":"Index","title":"ActiveInference.Environments.bayesian_model_average","text":"Calculate Bayesian Model Average (BMA)\n\nCalculates the Bayesian Model Average (BMA) which is used for the State Action Prediction Error (SAPE). It is a weighted average of the expected states for all policies weighted by the posterior over policies. The qs_pi_all should be the collection of expected states given all policies. Can be retrieved with the get_expected_states function.\n\nqs_pi_all: Vector{Any} \n\nq_pi: Vector{Float64}\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.calculate_bayesian_surprise-Tuple{Any, Any}","page":"Index","title":"ActiveInference.Environments.calculate_bayesian_surprise","text":"Calculate Bayesian Surprise \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.capped_log-Tuple{Array{Float64}}","page":"Index","title":"ActiveInference.Environments.capped_log","text":"capped_log(array::Array{Float64})\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.capped_log-Tuple{Real}","page":"Index","title":"ActiveInference.Environments.capped_log","text":"capped_log(x::Real)\n\nArguments\n\nx::Real: A real number.\n\nReturn the natural logarithm of x, capped at the machine epsilon value of x.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.capped_log-Tuple{Vector{Real}}","page":"Index","title":"ActiveInference.Environments.capped_log","text":"capped_log(array::Vector{Real})\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.capped_log-Union{Tuple{Array{T}}, Tuple{T}} where T<:Real","page":"Index","title":"ActiveInference.Environments.capped_log","text":"capped_log(array::Array{T}) where T <: Real\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.capped_log_array-Tuple{Any}","page":"Index","title":"ActiveInference.Environments.capped_log_array","text":"Apply capped_log to array of arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.dot_likelihood-Tuple{Any, Any}","page":"Index","title":"ActiveInference.Environments.dot_likelihood","text":"Dot-Product Function \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.get_joint_likelihood-Tuple{Any, Any, Any}","page":"Index","title":"ActiveInference.Environments.get_joint_likelihood","text":"Get Joint Likelihood \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.kl_divergence-Tuple{Vector{Vector{Vector{Real}}}, Vector{Vector{Vector{Real}}}}","page":"Index","title":"ActiveInference.Environments.kl_divergence","text":"kl_divergence(P::Vector{Vector{Vector{Float64}}}, Q::Vector{Vector{Vector{Float64}}})\n\nArguments\n\nP::Vector{Vector{Vector{Real}}}\nQ::Vector{Vector{Vector{Real}}}\n\nReturn the Kullback-Leibler (KL) divergence between two probability distributions.\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.normalize_arrays-Tuple{Vector{<:Array{<:Real}}}","page":"Index","title":"ActiveInference.Environments.normalize_arrays","text":"Normalizes multiple arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.normalize_arrays-Tuple{Vector{Any}}","page":"Index","title":"ActiveInference.Environments.normalize_arrays","text":"Normalizes multiple arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.normalize_distribution-Tuple{Any}","page":"Index","title":"ActiveInference.Environments.normalize_distribution","text":"Normalizes a Categorical probability distribution\n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.outer_product","page":"Index","title":"ActiveInference.Environments.outer_product","text":"Multi-dimensional outer product \n\n\n\n\n\n","category":"function"},{"location":"#ActiveInference.Environments.softmax_array-Tuple{Any}","page":"Index","title":"ActiveInference.Environments.softmax_array","text":"Softmax Function for array of arrays \n\n\n\n\n\n","category":"method"},{"location":"#ActiveInference.Environments.spm_wnorm-Tuple{Any}","page":"Index","title":"ActiveInference.Environments.spm_wnorm","text":"SPM_wnorm \n\n\n\n\n\n","category":"method"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"EditURL = \"../julia_files/AgentCreation.jl\"","category":"page"},{"location":"AgentCreation/#Creating-the-Agent","page":"Creating the Agent","title":"Creating the Agent","text":"","category":"section"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"Having created the generative model parameters in the precious section, we're not ready to intialise an active inference agent. Firstly, we'll have to specify some settings and hyperparameters that go into the agent struct. We'll begin with the setting:","category":"page"},{"location":"AgentCreation/#Settings","page":"Creating the Agent","title":"Settings","text":"","category":"section"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"The settings are a dictionary that contains the following keys:","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"settings = Dict(\n    \"policy_len\" => 1,\n    \"use_utility\" => true,\n    \"use_states_info_gain\" => true,\n    \"use_param_info_gain\" => false,\n    \"action_selection\" => \"stochastic\",\n    \"modalities_to_learn\" => \"all\",\n    \"factors_to_learn\" => \"all\",\n    \"FPI_num_iter\" => 10,\n    \"FPI_dF_tol\" => 0.001\n)","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"The above shown values are the default and will work in most cases. If you're unsure about what to specify in the settings, you can just use the default values by not specifying them in the settings Dict for the agent. Here, we'll briefly describe the keys in the settings dictionary:","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"policy_len - Is the policy length, and as described previously is the number of actions the agent should plan in the future. This is provided as an integer.\nuse_utility - Is a boolean that specifies whether the agent should use C in the expected free energy calculation, that guides the action selection in active inference. If set to false, the agent will not use the parameters specified in C.\nuse_states_info_gain - Is a boolean that specifies whether the agent should use the information gain over states in the expected free energy calculation. If set to false, the agent will not use the information gain over states.\nuse_param_info_gain - Is a boolean that specifies whether the agent should use the information gain over parameters in the expected free energy calculation. If set to false, the agent will not use the information gain over parameters. Only relevant when learning is included.\naction_selection - Is a string that specifies the action selection method. The options are \"stochastic\" and \"deterministic\". If set to \"stochastic\", the agent will sample from the posterior over policies, and if set to \"deterministic\", the agent will choose the most probable action.\nmodalities_to_learn - Is a vector of integers that specifies which modalities the agent should learn. If set to string \"all\", the agent will learn all modalities. If set to [1,2], the agent will only learn the first and second modality. Only relevant when learning of A is included.\nfactors_to_learn - Is a vector of integers that specifies which factors the agent should learn. If set to string \"all\", the agent will learn all factors. If set to [1,2], the agent will only learn the first and second factor. Only relevant when learning of B and D is included.\nFPI_num_iter - Is an integer that specifies the number of fixed point iterations (FPI) to perform in the free energy minimization. It can be described as a stop function of the FPI algorithm.\nFPI_dF_tol - Is a float that specifies the tolerance of the free energy change in the FPI algorithm over each iteration. If the change in free energy is below this value, the FPI algorithm will also stop.","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"For more information on the specifics of the impact of these settings, look under the Active Inference Theory section in the documentation.","category":"page"},{"location":"AgentCreation/#Parameters","page":"Creating the Agent","title":"Parameters","text":"","category":"section"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"The parameters are a dictionary that contains the following keys:","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"parameters = Dict(\n\"gamma\" => 16.0,\n\"alpha\" => 16.0,\n\"lr_pA\" => 1.0,\n\"fr_pA\" => 1.0,\n\"lr_pB\" => 1.0,\n\"fr_pB\" => 1.0,\n\"lr_pD\" => 1.0,\n\"fr_pD\" => 1.0\n)","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"The above shown values are the default. If you're unsure about what to specify in the parameters, you can just use the default values by not specifying them in the parameter Dict for the agent. Here, we'll briefly describe the keys in the parameters dictionary containing the hyperparameters:","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"alpha - Is the inverse temperature of the action selection process, and usually takes a value between 1 and 32. This is only relevant when action_selection is set to \"stochastic\".\ngamma - Is the inverse temperature precision of the expected free energy, and usually takes a value between 1 and 32. If the value is high, the agent will be more certain in its beliefs regarding the posterior probability over policies.\nlr_pA - Is the learning rate of A, and usually takes a value between 0 and 1. Only relevant when learning is included, and this goes for all learning and forgetting rates.\nfr_pA - Is the forgetting rate of A, and usually takes a value between 0 and 1. If forgetting rate is 1 it means no forgetting.\nlr_pB - Is the learning rate of B, and usually takes a value between 0 and 1.\nfr_pB - Is the forgetting rate of B, and usually takes a value between 0 and 1. If forgetting rate is 1 it means no forgetting.\nlr_pD - Is the learning rate of D, and usually takes a value between 0 and 1.\nfr_pD - Is the forgetting rate of D, and usually takes a value between 0 and 1. If forgetting rate is 1 it means no forgetting.","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"Having now specified the setting and parameters, we can now initialise the active inference agent. This is done by calling the init_aif function, which takes the following arguments:","category":"page"},{"location":"AgentCreation/#Initilising-the-Agent","page":"Creating the Agent","title":"Initilising the Agent","text":"","category":"section"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"aif_agent = init_aif(\n    A, B, C = C, D = D, E = E, settings = settings, parameters = parameters, verbose = false\n);","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"You can access the settings and parameters of the agent by calling the agent struct on the agent:","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"aif_agent.parameters","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"Dict{String, Real} with 8 entries:\n  \"lr_pA\" => 1.0\n  \"fr_pA\" => 1.0\n  \"lr_pB\" => 1.0\n  \"lr_pD\" => 1.0\n  \"alpha\" => 16.0\n  \"gamma\" => 16.0\n  \"fr_pB\" => 1.0\n  \"fr_pD\" => 1.0","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"aif_agent.settings","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"Dict{String, Any} with 11 entries:\n  \"policy_len\" => 1\n  \"FPI_dF_tol\" => 0.001\n  \"control_fac_idx\" => [1]\n  \"action_selection\" => \"stochastic\"\n  \"num_controls\" => [4, 1]\n  \"FPI_num_iter\" => 10\n  \"modalities_to_learn\" => \"all\"\n  \"use_utility\" => true\n  \"factors_to_learn\" => \"all\"\n  \"use_param_info_gain\" => false\n  \"use_states_info_gain\" => true","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"Having now initialised the agent, we are ready to implement it either in a simulation with a perception-action loop, or for use in model fitting with observed data.","category":"page"},{"location":"AgentCreation/#Initialising-the-Agent-with-Learning","page":"Creating the Agent","title":"Initialising the Agent with Learning","text":"","category":"section"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"If you want to include learning in the agent, you can do so by specifying the prior parameters init_aif function. Here is an example of how to initialise the agent with learning:","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"aif_agent = init_aif(\n    A, B, C = C, D = D, E = E, pA = pA, pB = pB, pD = pD, settings = settings, parameters = parameters, verbose = false\n);","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"Here, only the prior of the parameters that are to be learned should be specified.","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"","category":"page"},{"location":"AgentCreation/","page":"Creating the Agent","title":"Creating the Agent","text":"This page was generated using Literate.jl.","category":"page"}]
}
