<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>T-Maze Simulation · ActiveInference.jl</title><meta name="title" content="T-Maze Simulation · ActiveInference.jl"/><meta property="og:title" content="T-Maze Simulation · ActiveInference.jl"/><meta property="twitter:title" content="T-Maze Simulation · ActiveInference.jl"/><meta name="description" content="Documentation for ActiveInference.jl."/><meta property="og:description" content="Documentation for ActiveInference.jl."/><meta property="twitter:description" content="Documentation for ActiveInference.jl."/><meta property="og:url" content="https://ilabcode.github.io/ActiveInference.jl/TMazeSimulationExample/"/><meta property="twitter:url" content="https://ilabcode.github.io/ActiveInference.jl/TMazeSimulationExample/"/><link rel="canonical" href="https://ilabcode.github.io/ActiveInference.jl/TMazeSimulationExample/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../Introduction/">ActiveInference.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">General Introduction</span><ul><li><a class="tocitem" href="../Introduction/">Introduction</a></li><li><a class="tocitem" href="../GenerativeModelCreation/">Creation of the Generative Model</a></li><li><a class="tocitem" href="../AgentCreation/">Creating the Agent</a></li><li><a class="tocitem" href="../Simulation/">Simulation</a></li><li><a class="tocitem" href="../Fitting/">Model Fitting</a></li><li><a class="tocitem" href="../SimulationActionModels/">Simulation with ActionModels.jl</a></li></ul></li><li><span class="tocitem">Usage Examples</span><ul><li class="is-active"><a class="tocitem" href>T-Maze Simulation</a></li></ul></li><li><span class="tocitem">Theory</span><ul><li><a class="tocitem" href="../GenerativeModelTheory/">POMDP Theory</a></li></ul></li><li><a class="tocitem" href="../">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Usage Examples</a></li><li class="is-active"><a href>T-Maze Simulation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>T-Maze Simulation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ilabcode/ActiveInference.jl/blob/master/docs/julia_files/TMazeSimulationExample.jl#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Simulation-Example-T-Maze"><a class="docs-heading-anchor" href="#Simulation-Example-T-Maze">Simulation Example T-Maze</a><a id="Simulation-Example-T-Maze-1"></a><a class="docs-heading-anchor-permalink" href="#Simulation-Example-T-Maze" title="Permalink"></a></h1><p>We will start from the importing of the necessary modules.</p><pre><code class="language-julia hljs">using ActiveInference
using ActiveInference.Environments</code></pre><p>We will create a T-Maze environment with a probability of 0.9 for reward in the the reward condition arm. This is a premade environment in the ActiveInference.jl package.</p><pre><code class="language-julia hljs">env = TMazeEnv(0.9)
initialize_gp(env)</code></pre><h3 id="Creating-the-Generative-Model"><a class="docs-heading-anchor" href="#Creating-the-Generative-Model">Creating the Generative Model</a><a id="Creating-the-Generative-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-the-Generative-Model" title="Permalink"></a></h3><h4 id="The-Helper-Function"><a class="docs-heading-anchor" href="#The-Helper-Function">The Helper Function</a><a id="The-Helper-Function-1"></a><a class="docs-heading-anchor-permalink" href="#The-Helper-Function" title="Permalink"></a></h4><p>When creating the generative model we can make use of the helper function, making it convenient to create the correct structure for the generative model parameters.</p><p>To use the helper function we need to know the following:</p><ul><li>Number of states in each factor of the environment</li><li>Number of observations in each modality</li><li>Number of controls or actions in each factor</li><li>Policy length of the agent</li><li>Initial fill for the parameters</li></ul><p>Let&#39;s start with the factors of the environment. Let&#39;s take a look at the T-Maze environment:</p><p><img src="../assets/TMazeIllustrationSmaller.png" alt="image1"/></p><p>We here have two factors with the following number of states:</p><table><tr><th style="text-align: left"></th><th style="text-align: left">Location Factor</th><th style="text-align: left"></th><th style="text-align: left">Reward Condition Factor</th></tr><tr><td style="text-align: left">1.</td><td style="text-align: left">Centre</td><td style="text-align: left">1.</td><td style="text-align: left">Reward Condition Left</td></tr><tr><td style="text-align: left">2.</td><td style="text-align: left">Left Arm</td><td style="text-align: left">2.</td><td style="text-align: left">Reward Condition Right</td></tr><tr><td style="text-align: left">3.</td><td style="text-align: left">Right Arm</td><td style="text-align: left"></td><td style="text-align: left"></td></tr><tr><td style="text-align: left">4.</td><td style="text-align: left">Cue</td><td style="text-align: left"></td><td style="text-align: left"></td></tr></table><p>We will define this as a vector the following way:</p><pre><code class="language-julia hljs">n_states = [4, 2]</code></pre><p>We will now define the modalities:</p><table><tr><th style="text-align: left"></th><th style="text-align: left">Location Modality</th><th style="text-align: left"></th><th style="text-align: left">Reward Modality</th><th style="text-align: left"></th><th style="text-align: left">Cue Modality</th></tr><tr><td style="text-align: left">1.</td><td style="text-align: left">Centre</td><td style="text-align: left">1.</td><td style="text-align: left">No Reward</td><td style="text-align: left">1.</td><td style="text-align: left">Cue Left</td></tr><tr><td style="text-align: left">2.</td><td style="text-align: left">Left Arm</td><td style="text-align: left">2.</td><td style="text-align: left">Reward</td><td style="text-align: left">2.</td><td style="text-align: left">Cue Right</td></tr><tr><td style="text-align: left">3.</td><td style="text-align: left">Right Arm</td><td style="text-align: left">3.</td><td style="text-align: left">Loss</td><td style="text-align: left"></td><td style="text-align: left"></td></tr><tr><td style="text-align: left">4.</td><td style="text-align: left">Cue</td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td><td style="text-align: left"></td></tr></table><p>Here we have 3 modalities, with 4, 3, and 2 observations in each. We will define this as a vector the following way:</p><pre><code class="language-julia hljs">n_observations = [4, 3, 2]</code></pre><p>Now, let&#39;s take a look at the actions, or controls:</p><table><tr><th style="text-align: left"></th><th style="text-align: left">Controls Location Factor</th><th style="text-align: left"></th><th style="text-align: left">Controls Reward Condition Factor</th></tr><tr><td style="text-align: left">1.</td><td style="text-align: left">Go to Centre</td><td style="text-align: left">1.</td><td style="text-align: left">No Control</td></tr><tr><td style="text-align: left">2.</td><td style="text-align: left">Go to Left Arm</td><td style="text-align: left"></td><td style="text-align: left"></td></tr><tr><td style="text-align: left">3.</td><td style="text-align: left">Go to Right Arm</td><td style="text-align: left"></td><td style="text-align: left"></td></tr><tr><td style="text-align: left">4.</td><td style="text-align: left">Go to Cue</td><td style="text-align: left"></td><td style="text-align: left"></td></tr></table><p>As we see here, the agent cannot control the reward condition factor, and it therefore believes that there is only one way states can transition in this factor, which is independent of the agent&#39;s actions. We will define this as a vector the following way:</p><pre><code class="language-julia hljs">n_controls = [4, 1]</code></pre><p>Now we can define the policy length of the agent. In this case we will just set it to 2, meaning that the agent plans two timesteps ahead in the future. We will just specify this as an integer:</p><pre><code class="language-julia hljs">policy_length = 2</code></pre><p>The last thing we need to define is the initial fill for the parameters. We will just set this to zeros for now.</p><pre><code class="language-julia hljs">template_type = &quot;zeros&quot;</code></pre><p>Having defined all the arguments that go into the helper function, we can now create the templates for the generative model parameters.</p><pre><code class="language-julia hljs">A, B, C, D, E = create_matrix_templates(n_states, n_observations, n_controls, policy_length, template_type);</code></pre><h4 id="Populating-the-Generative-Model"><a class="docs-heading-anchor" href="#Populating-the-Generative-Model">Populating the Generative Model</a><a id="Populating-the-Generative-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Populating-the-Generative-Model" title="Permalink"></a></h4><h5 id="Populating-**A**"><a class="docs-heading-anchor" href="#Populating-**A**">Populating <strong>A</strong></a><a id="Populating-**A**-1"></a><a class="docs-heading-anchor-permalink" href="#Populating-**A**" title="Permalink"></a></h5><p>Let&#39;s take a look at the shape of the first modality in the A parameters:</p><pre><code class="language-julia hljs">A[1]</code></pre><pre><code class="nohighlight hljs">4×4×2 Array{Float64, 3}:
[:, :, 1] =
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0

[:, :, 2] =
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0</code></pre><p>For this first modality we provide the agent with certain knowledge on how location observations map onto location states. We do this the following way:</p><pre><code class="language-julia hljs"># For reward condition right
A[1][:,:,1] = [ 1.0  0.0  0.0  0.0
                0.0  1.0  0.0  0.0
                0.0  0.0  1.0  0.0
                0.0  0.0  0.0  1.0 ]

# For reward condition left
A[1][:,:,2] = [ 1.0  0.0  0.0  0.0
                0.0  1.0  0.0  0.0
                0.0  0.0  1.0  0.0
                0.0  0.0  0.0  1.0 ]</code></pre><p>For the second modality, the reward modality, we want the agent to be able to infer &quot;no reward&quot; with certainty when in the centre and cue locations. In the left and right arm though, the agent should be agnostic as to which arm produces reward and loss. This is the modality that will be learned in this example.</p><pre><code class="language-julia hljs"># For reward condition right
A[2][:,:,1] = [ 1.0  0.0  0.0  1.0
                0.0  0.5  0.5  0.0
                0.0  0.5  0.5  0.0 ]

# For reward condition left
A[2][:,:,2] = [ 1.0  0.0  0.0  1.0
                0.0  0.5  0.5  0.0
                0.0  0.5  0.5  0.0 ]</code></pre><p>In the third modality, we want the agent to infer the reward condition state when in the cue location. To do this, we give it an uniform probability for all locations except the cue location, where it veridically will observe the reward condition state.</p><pre><code class="language-julia hljs"># For reward condition right
A[3][:,:,1] = [ 0.5  0.5  0.5  1.0
                0.5  0.5  0.5  0.0 ]

# For reward condition left
A[3][:,:,2] = [ 0.5  0.5  0.5  0.0
                0.5  0.5  0.5  1.0 ]</code></pre><h5 id="Populating-**B**"><a class="docs-heading-anchor" href="#Populating-**B**">Populating <strong>B</strong></a><a id="Populating-**B**-1"></a><a class="docs-heading-anchor-permalink" href="#Populating-**B**" title="Permalink"></a></h5><p>For the first factor we populate the <strong>B</strong> with determined beliefs about how the location states change depended on its actions. For each action, it determines where to go, without having to go through any of the other location states. We encode this as:</p><pre><code class="language-julia hljs"># For action &quot;Go to Center Location&quot;
B[1][:,:,1] = [ 1.0  1.0  1.0  1.0
                0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0 ]

# For action &quot;Go to Right Arm&quot;
B[1][:,:,2] = [ 0.0  0.0  0.0  0.0
                1.0  1.0  1.0  1.0
                0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0 ]

# For action &quot;Go to Left Arm&quot;
B[1][:,:,3] = [ 0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0
                1.0  1.0  1.0  1.0
                0.0  0.0  0.0  0.0 ]

# For action &quot;Go to Cue Location&quot;
B[1][:,:,4] = [ 0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0
                0.0  0.0  0.0  0.0
                1.0  1.0  1.0  1.0 ]</code></pre><p>For the last factor there is no control, so we will just set the <strong>B</strong> to be the identity matrix.</p><pre><code class="language-julia hljs"># For second factor, which is not controlable by the agent
B[2][:,:,1] = [ 1.0  0.0
                0.0  1.0 ]</code></pre><h5 id="Populating-**C**"><a class="docs-heading-anchor" href="#Populating-**C**">Populating <strong>C</strong></a><a id="Populating-**C**-1"></a><a class="docs-heading-anchor-permalink" href="#Populating-**C**" title="Permalink"></a></h5><p>For the preference parameters <strong>C</strong> we are not interested in the first and third modality, which we will just set to a vector of zeros for each observation in that modality. However, for the second modality, we want the agent to prefer the &quot;reward observation&quot; indexed as 2, and the dislike the &quot;loss observation&quot; indexed as 3.</p><pre><code class="language-julia hljs"># Preference over locations modality
C[1] = [0.0, 0.0, 0.0, 0.0]

# Preference over reward modality
C[2] = [0.0, 3.0, -3.0]

# Preference over cue modality
C[3] = [0.0, 0.0]</code></pre><h5 id="Populating-**D**"><a class="docs-heading-anchor" href="#Populating-**D**">Populating <strong>D</strong></a><a id="Populating-**D**-1"></a><a class="docs-heading-anchor-permalink" href="#Populating-**D**" title="Permalink"></a></h5><p>For the prior over states <strong>D</strong> we will set the agent&#39;s belief to be correct in the location state factor and uniform, or agnostic, in the reward condition factor.</p><pre><code class="language-julia hljs"># For the location state factor
D[1] = [1.0, 0.0, 0.0, 0.0]

# For the reward condition state factor
D[2] = [0.5, 0.5]</code></pre><h5 id="Populating-**E**"><a class="docs-heading-anchor" href="#Populating-**E**">Populating <strong>E</strong></a><a id="Populating-**E**-1"></a><a class="docs-heading-anchor-permalink" href="#Populating-**E**" title="Permalink"></a></h5><p>For the prior over policies <strong>E</strong> we will set it to be uniform, meaning that the agent has no prior preference for any policy.</p><pre><code class="language-julia hljs"># Creating a vector of a uniform distribution over the policies. This means no preferences over policies.
E .= 1.0/length(E)</code></pre><h5 id="Creating-the-prior-over-**A**"><a class="docs-heading-anchor" href="#Creating-the-prior-over-**A**">Creating the prior over <strong>A</strong></a><a id="Creating-the-prior-over-**A**-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-the-prior-over-**A**" title="Permalink"></a></h5><p>When creating the prior over <strong>A</strong>, we use <strong>A</strong> as a template, by using &#39;deepcopy()&#39;. Then we multiply this with a scaling parameter, setting the initial concentration parameters for the Dirichlet prior over <strong>A</strong>, <strong>pA</strong>.</p><pre><code class="language-julia hljs">pA = deepcopy(A)
scale_concentration_parameter = 2.0
pA .*= scale_concentration_parameter</code></pre><h4 id="Creating-Settings-and-Parameters-Dictionary"><a class="docs-heading-anchor" href="#Creating-Settings-and-Parameters-Dictionary">Creating Settings and Parameters Dictionary</a><a id="Creating-Settings-and-Parameters-Dictionary-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-Settings-and-Parameters-Dictionary" title="Permalink"></a></h4><p>For the settings we set the &#39;use<em>param</em>info<em>gain&#39; and &#39;use</em>states<em>info</em>gain&#39; to true, meaning that the agent will take exploration and parameter learning into account when calculating the prior over policies. We set the policy length to 2, and specify modalities to learn, which in our case is the reward modality, indexed as 2.</p><pre><code class="language-julia hljs">settings = Dict(
    &quot;use_param_info_gain&quot; =&gt; true,
    &quot;use_states_info_gain&quot; =&gt; true,
    &quot;policy_len&quot; =&gt; 2,
    &quot;modalities_to_learn&quot; =&gt; [2]
)</code></pre><p>For the parameters, we just use the default values, but specify the learning rate here, just to point it out.</p><pre><code class="language-julia hljs">parameters = Dict{String, Real}(
    &quot;lr_pA&quot; =&gt; 1.0,
)</code></pre><h3 id="Initilising-the-Agent"><a class="docs-heading-anchor" href="#Initilising-the-Agent">Initilising the Agent</a><a id="Initilising-the-Agent-1"></a><a class="docs-heading-anchor-permalink" href="#Initilising-the-Agent" title="Permalink"></a></h3><p>We can now initialise the agent with the parameters and settings we have just specified.</p><pre><code class="language-julia hljs">aif_agent = init_aif(
    A, B, C = C, D = D, E = E, pA = pA, settings = settings, parameters = parameters
);</code></pre><h3 id="Simulation"><a class="docs-heading-anchor" href="#Simulation">Simulation</a><a id="Simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Simulation" title="Permalink"></a></h3><p>We are now ready for the perception-action-learning loop:</p><pre><code class="language-julia hljs"># Settting the number of trials
T = 100

# Creating an initial observation and resetting environment (reward condition might change)
obs = reset_TMaze!(Env)

# Creating a for-loop that loops over the perception-action-learning loop T amount of times
for t = 1:T

    # Infer states based on the current observation
    infer_states!(aif_agent, obs)

    # Updates the A parameters
    update_parameters!(aif_agent)

    # Infer policies and calculate expected free energy
    infer_policies!(aif_agent)

    # Sample an action based on the inferred policies
    chosen_action = sample_action!(aif_agent)

    # Feed the action into the environment and get new observation.
    obs = step_TMaze!(Env, chosen_action)
end</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../SimulationActionModels/">« Simulation with ActionModels.jl</a><a class="docs-footer-nextpage" href="../GenerativeModelTheory/">POMDP Theory »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 5 December 2024 19:01">Thursday 5 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
